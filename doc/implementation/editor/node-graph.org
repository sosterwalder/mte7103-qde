The functionality of the node graph is, as its name states, to represent a data
structure composed of nodes and edges. Each scene from the scene graph is
represented within the node graph as such a data structure.

The nodes are the building blocks of a real time animation. They represent
different aspects, such as scenes themselves, time line clips, models, cameras,
lights, materials, generic operators and effects. These aspects are only examples
(coming from \cite[p. 30 and 31]{osterwalder_qde_2016}) as the node structure
must be expandable for allowing the addition of new nodes.

The implementation of the scene graph component was relatively straightforward
partly due to its structure and partly due to the used data model and
representation. The node graph component however, seems to be a bit more complex.

To get a first overview and to manage its complexity, it might be good to
identify its sub components first before implementing them.
When thinking about the implementation of the node graph, one may identify the
following sub components:

- Nodes
  - Domain model :: Holds data of a node, like its definition, its inputs and so
                    on.
  - Definitions  :: Represents a domain model as JSON data structure.
  - Controller   :: Handles the loading of node definitions as well as the
                    creation of node instances.
  - View model   :: Represents a node within the graphical user interface.
- Scenes
  - Domain model :: Holds the data of a scene, e.g. its nodes.
  - Controller   :: Handles scene related actions, like when a node is added to
                    a scene, when the scene was changed or when a node within a
                    scene was selected.
  - View model   :: Defines the graphical representation of scene which can be
                    represented by the corresponding view. Basically the scene
                    view model is a canvas consisting of nodes.
  - View         :: Represents scenes in terms of scene view models within the
                    graphical user interface.

**** Nodes

One of the most basic sub components are the definitions of nodes. But what are
those definitions actually? What do they actually define? There is not only one
answer to this question, it is simply a matter of how the implementation is
being done and therefore a set of decisions.

The whole (rendering) system shall not be bound to only one representation of
nodes, e.g. triangle based meshes. Instead it shall let the user decide, what
representation is the most fitting for the goal he wants to achieve.

At this point the system shall be able to theoretically support multiple kinds of
node representations: Images, triangle based meshes and solid modeling through
function modeling (using signed distance functions for modeling implicit
surfaces). Whereas triangle based meshes may either be loaded from externally
defined files (e.g. in the Filmbox (FBX), the Alembic (ABC) or the Object file
format (OBJ)) or directly be generated using procedural mesh generation.

When implementing the nodes, keep in mind that the nodes are part of a graph,
hence the name node graph, and are therefore typically connected by edges. This
means that the graph gets evaluated recursively by its nodes. However, the goal
is to have OpenGL shading language (GLSL) code at the end, independent of the
node types.

From this point of view it is theoretically possible to let the user define shader
code directly within a node (definition) and to simply evaluate this code, which
adds a lot of (creative) freedom. The problem with this approach is though, that
image and triangle based mesh nodes are not fully implementable by using shader
code only. Instead they have specific requirements, which are only perform-able
on the CPU instead of the GPU (e.g. allocating buffer objects).

When thinking of nodes used for solid modeling however, it may appear, that they
may be evaluated directly, without the need for pre-processing, as they are
fully implementable using shader code only. This is kind of misleading however
as each node has its own definition which has to be added to shader and this
definition is then used in a mapping function to compose the scene. This would
mean to add a definition of a node over and over again, when spawning multiple
instances of the same node type, which results in overhead bloating the shader.
It is therefore necessary to pre-process solid modeling nodes too, exactly as
triangle mesh based and image nodes, for being able to use multiple instances of
the same node type within a scene while having the definition added only once.

All of these thoughts sum up in one central question for the implementation:
Shall objects be predefined within the code (and therefore only nodes accepted
whose type and sub type match those of predefined nodes) or shall all objects be
defined externally using files?

This is a question which is not that easy to answer. Both methods have their
advantages and disadvantages. Pre-defining nodes within the code minimizes
unexpected behavior of the application. Only known and well-defined nodes are
processed.

But what if someone would like to have a new node type which is not yet defined?
The node type has to be implemented first. As Python is used for the editor
application, this is not really a problem as the code is interpreted each time
and is therefore not being compiled. Nevertheless such changes follow a certain
process, such as making the actual changes, reviewing and checking-in the code
and so on, which the user normally does not want to be bothered with.
Furthermore, when thinking about the player application, the problem of the
necessity to recompile the code is definitively given. The player will be
implemented in C, as there is the need for performance, which Python may not
fulfill satisfactorily.

Considering these aspects, the external definition of nodes is chosen. This may
result in nodes which cannot be evaluated or which have unwanted effects. As it
is (most likely) in the users best interest to create (for his taste) appealing
real time animations, it can be assumed, that the user will try avoiding to
create such nodes or quickly correct faulty nodes or simply does not use such
nodes.

Now, having chosen how to implement nodes, let us define what a node actually
is. A node is essentially composed by the following elements:

- ID          :: A global unique identifier (UUID
                 [fn:7745e3126d80f1a:https://docs.python.org/3/library/uuid.html])
- Name        :: The name of the node, e.g. "Cube".
- Inputs      :: A list of the nodes inputs. The inputs may either be parameters
                 (which are atomic types such as float values or text input)
                 or references to other nodes.
- Outputs     :: A list of the nodes outputs. The outputs may also either be
                 parameters or references to other nodes.
- Parts       :: Defines parts that may be evaluated when evaluating the node.
                 Contains code which can be interpreted directly.
- Nodes       :: The children a node has (child nodes). These entries are
                 references to other nodes only.
- Connections :: A list of connections between the input and output of the node.
                 Each input and output is composed by two parts: A reference to
                 another node and a reference to that nodes output or input
                 respectively.
                 Input: Reference to a node X + Reference to /output/ A of node X.
                 Output: Reference to a node Y + Reference to /input/ B of node Y.

As can be derived from the above thoughts, each of the mentioned node
representations need some effort in terms of allocating buffers or render
targets before they may be used for rendering a frame. They may as well want to
free or release some of their made allocations when not being used anymore.

In other words, every node will be pre-processed before being processed and
post-processed after being processed.

To keep the learning curve at a decent level when using the editor application,
it is important to provide the user with predefined nodes to choose from ---
independent from their type --- otherwise users could get easily frustrated at
the beginning. The following nodes are envisaged:

- Solid modeling objects
  - Sphere
  - Cube
  - Plane
  - ...
- Solid modeling operations
  - Transformation
  - Scaling
  - Rotation
  - Union
  - Differentiation
  - ...
- Post-processing effects
  - Blur
  - Glare
  - ...
- Images

To get the node graph implementation started, a sample node definition is
implemented as well as its defining class.

**** Node definitions

Node definitions are implemented in the
JSON[fn:9d5e4e40b523c9e:http://www.ecma-international.org/publications/files/ECMA-ST/Ecma-262.pdf]
format and are placed in the =data/nodes/= sub-directory, seen from the main
directory.

:THOUGHTS_BEGIN:
ParameterID defines a globally unique ID and is used for inputs and outputs.
It may however come from the ID of an actual node, then the input is another node.
If it does not come from another node, the input is a basic type.

An input defines a parameter and may be referenced as output as well.

Every input and output receives a ParameterInstanceID, which is globally unique
as well. Such an ID is used for making connections between nodes and/or parts of
nodes. Parts = Code?

A node may contain other nodes as well, those are references however.

A few node types are pre-defined however. Those can be selected as input and as
output as well. The pre-defined types are:

- Generic
- Float (value)
- Text
- Scene
- Image
- Dynamic (value)
- Mesh

Connections are actually parts.
We need a context. Nodes get processed using this (global) context.
:END:
